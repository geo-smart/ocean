{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7298eb1-3264-4415-bc2b-b2351e97e8da",
   "metadata": {},
   "source": [
    "# Duplicate timestamps in THREDDS 1Min data\n",
    "\n",
    "\n",
    "In the `data.ipynb` notebook the goals are:\n",
    "- Ingest low-time-resolution data from the THREDDS server\n",
    "    - 1 sample per minute\n",
    "    - 17 data files in NetCDF format, 23 sensors\n",
    "    - Select a time window (e.g. one month: Jan 2022)\n",
    "    - Write to results as NetCDF files to a local (in-repo) data folder\n",
    "- Check the data for expected profiler behavior based on the `z` depth data variable\n",
    "- Extract a timestamp dataset (table > CSV file) for ascents, descents, and rest intervals \n",
    "\n",
    "\n",
    "It proved to be the case that the 1Min data often had multiple values for a given timestamp. \n",
    "I introduced a solution to the processing chain; and did an in-place fix for the initial dataset. \n",
    "\n",
    "\n",
    "***The main issue is: When timestamps are duplicated: Sensor values vary. They are not duplicates. Why???***\n",
    "\n",
    "\n",
    "***Lesser issue: Why does a loop over Dataset times bog down and not complete? (see 'bog down' below)***\n",
    "\n",
    "\n",
    "\n",
    "Continuing to the fix: Presuming 1Min data for one month, January 2022amounts to 31 * 24 * 60 = 44640 samples. \n",
    "Survey the 17 data files for duplicates:\n",
    "\n",
    "\n",
    "```\n",
    "print(xr.open_dataset('../data/osb_do_jan22_do.nc'))\n",
    "etcetera...\n",
    "```\n",
    "\n",
    "\n",
    "Results: The first six sets clearly have many duplicates. The first number >> 44640.\n",
    "\n",
    "\n",
    "- 146283 ctd { density, pressure, salinity, temperature, conductivity }\n",
    "- 146206 do\n",
    "- 138837 fluor { bb, chlora, fdom }\n",
    "- 146185 par\n",
    "- 147451 7 x spkir (one file) { 412, 443, 490, 510, 555, 620, 683 nm }\n",
    "- 149216 current (three files) { east, north, up }\n",
    "- 998  pco2 had no duplicates\n",
    "- 1005 ph, had 2 duplicates\n",
    "- 6399 nitrate, duplicates present\n",
    "\n",
    "\n",
    "These two lines of code remove duplicate timestamps from a Dataset:\n",
    "\n",
    "\n",
    "```\n",
    "_, keeper_index = np.unique(ds['time'], return_index=True)\n",
    "ds=ds.isel(time=keeper_index)\n",
    "```\n",
    "\n",
    "\n",
    "#### Code explanation\n",
    "\n",
    "`np.unique()` with the `True` argument returns a tuple (t, i): t is a sorted version of \n",
    "the `time` dimension as a numpy array and i is a list of unique indices. \n",
    "\n",
    "The sorted `time` we do not care about as it is only part of the Dataset. This is assigned\n",
    "to a `_` variable which is Python for \"I do not care\". \n",
    "\n",
    "The returned list of unique indices i will be those of the first occurrence of each \n",
    "timestamp, whether it is duplicated or not. This is assigned to `keeper_index` and \n",
    "that, in turn, is used in the following `.isel()` subset operation, applied to the \n",
    "entire Dataset:The `time` dimension is subsetted as are coordinates and data variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51329930-910e-4e09-8b5e-e41b89da6e4a",
   "metadata": {},
   "source": [
    "#### Code to investigate duplication of timestamps\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "ds_temperature = xr.open_dataset('../data/osb_ctd_jan22_temperature.nc')\n",
    "ds_ph        = xr.open_dataset('../data/osb_ph_jan22_ph.nc')\n",
    "\n",
    "len_ph = len(ds_ph['time'])\n",
    "len_temperature = len(ds_temperature['time'])\n",
    "\n",
    "ndup_ph = 0\n",
    "nvar_ph = 0\n",
    "ndup_temperature = 0\n",
    "nvar_temperature = 0\n",
    "\n",
    "for i in range(len_ph-1):\n",
    "    if ds_ph['time'][i] == ds_ph['time'][i+1]:\n",
    "        ndup_ph += 1\n",
    "        if not ds_ph['ph'][i] == ds_ph['ph'][i+1]:\n",
    "            nvar_ph += 1\n",
    "\n",
    "# Notice the code below only looks at the first 1000 samples, not all 140k. When trying\n",
    "#   to do all 140k the loop bogs down and fails to finish. This is strange as the data\n",
    "#   are thought to be sorted by time. Guess: Some JIT disk access pathology?\n",
    "for i in range(1000):\n",
    "    if ds_temperature['time'][i] == ds_temperature['time'][i+1]:\n",
    "        ndup_temperature += 1\n",
    "        if not ds_temperature['temperature'][i] == ds_temperature['temperature'][i+1]:\n",
    "            nvar_temperature += 1\n",
    "            \n",
    "print('ph:')\n",
    "print(len_ph, ndup_ph, nvar_ph)\n",
    "print()\n",
    "print('temperature')\n",
    "print(len_temperature, ndup_temperature, nvar_temperature)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6db881-f97f-4c3d-9bd7-af74d427e9b8",
   "metadata": {},
   "source": [
    "#### Remove duplicate timestamps\n",
    "\n",
    "\n",
    "This is free-standing but the important code is now in the main ingest function `ReformatDataFile()`\n",
    "found in `data.py`. \n",
    "\n",
    "\n",
    "```\n",
    "for s in jan22_data:\n",
    "    fnm = '../data/' + s\n",
    "    ds = xr.open_dataset(fnm)\n",
    "    len0 = len(ds['time'])\n",
    "    _, keeper_index = np.unique(ds['time'], return_index=True)\n",
    "    ds=ds.isel(time=keeper_index)\n",
    "    len1 = len(ds['time'])\n",
    "    ds.to_netcdf(fnm)\n",
    "    print(s + ' started with ' + str(len0) + ' timestamps, concluded with ' + str(len1))    \n",
    "\n",
    "output:\n",
    "    \n",
    "osb_ctd_jan22_pressure.nc started with 146283 timestamps, concluded with 44638\n",
    "osb_ctd_jan22_temperature.nc started with 146283 timestamps, concluded with 44638\n",
    "osb_ctd_jan22_density.nc started with 146283 timestamps, concluded with 44638\n",
    "osb_ctd_jan22_salinity.nc started with 146283 timestamps, concluded with 44638\n",
    "osb_ctd_jan22_conductivity.nc started with 146283 timestamps, concluded with 44638\n",
    "osb_fluor_jan22_fdom.nc started with 138837 timestamps, concluded with 43150\n",
    "osb_fluor_jan22_chlora.nc started with 138837 timestamps, concluded with 43150\n",
    "osb_fluor_jan22_bb.nc started with 138837 timestamps, concluded with 43150\n",
    "osb_spkir_jan22_spkir.nc started with 147451 timestamps, concluded with 44638\n",
    "osb_nitrate_jan22_nitrate.nc started with 6399 timestamps, concluded with 4101\n",
    "osb_pco2_jan22_pco2.nc started with 998 timestamps, concluded with 998\n",
    "osb_do_jan22_do.nc started with 146206 timestamps, concluded with 44627\n",
    "osb_par_jan22_par.nc started with 146185 timestamps, concluded with 44638\n",
    "osb_ph_jan22_ph.nc started with 1005 timestamps, concluded with 1003\n",
    "osb_vel_jan22_up.nc started with 149216 timestamps, concluded with 44638\n",
    "osb_vel_jan22_east.nc started with 149216 timestamps, concluded with 44638\n",
    "osb_vel_jan22_north.nc started with 149216 timestamps, concluded with 44638\n",
    "\n",
    "Typical file size dropped from 3MB to 900kb.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9a65e-39d8-453c-b667-7f8a2b048f36",
   "metadata": {},
   "source": [
    "# icepyx install is tetchy\n",
    "\n",
    "\n",
    "* I clone the `argo` branch: `git clone -b argo https://github.com/icesat2/icepyx`\n",
    "* I must install *from* the icepyx directory using `cd icepyx; pip install ./icepyx`\n",
    "* `import icepyx` works *from* a notebook located in the `icepyx` folder\n",
    "* Expect repaired on 1/1/2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a19405-617c-4a3f-a1c0-b44cf23863e8",
   "metadata": {},
   "source": [
    "## current velocity\n",
    "\n",
    "\n",
    "How is current velocity data generated? ADCP on the profiler platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9338599-1920-42be-a692-610fb8f56a6c",
   "metadata": {},
   "source": [
    "## bumpy data\n",
    "\n",
    "\n",
    "The reference data production must be modified to get away from sawtooth standard deviation profiles. These were a little apparent\n",
    "in `jan22` data and much more pronounced in `jul21` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc879b-63da-46f0-ad6f-20b69fafc0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
